#   Python 接入 Spark流程（old)
1.  下载spark  ==> https://spark.apache.org/downloads.html
2.  添加依赖库
    1.  解压找到spark/2.2.0/spark-2.2.0-bin-hadoop2.7/python/pyspark目录
    2.  在python依赖库目录如：/usr/local/lib/python2.7/site-packages 添加link 如：
```lrwxr-xr-x   1 seki  admin     73  1  6  2017 pyspark -> /Users/seki/work/lib/spark/2.2.0/spark-2.2.0-bin-hadoop2.7/python/pyspark
```
3.  运行环境，保证运行环境有/usr/local/lib/python2.7/site-packages
4.  运行方式
    4.1     代码运行  --https://spark.apache.org/docs/latest/sql-programming-guide.html
```
import matplotlib
matplotlib.use('TKAgg')
import numpy as np
import matplotlib.pyplot as plt

from pyspark import SparkConf   
from pyspark.context import SparkContext
from pyspark.sql import SparkSession

server_ip = "192.168.0.115"
url_local = "jdbc:mysql://"+ server_ip+":3306/hsp"
user_local = "root"
pass_local = "xifei"
day = 20170302

// 第一步初始化spark
spark = SparkSession \
    .builder \
    .master("spark://"+ server_ip +":7077") \ // spark集群master地址 现在应该为 spark://192.168.2.248:40070/
    .appName("python") \  // 程序在集群的运行的名称
    .config("spark.executor.memory", "20g") \ // 每个executor 启动内存
    .config("spark.executor.cores", "1")   // 每个executor 分配cpu数【不指定的情况，每个worker只有一个executor,使用所有分配的cpu】
    .config("spark.jars", "/Users/seki/.m2/repository/mysql/mysql-connector-java/5.1.34/mysql-connector-java-5.1.34.jar") \ // 运行时依赖jar,需要被发送的worker
    .config("spark.driver.extraClassPath", "/Users/seki/.m2/repository/mysql/mysql-connector-java/5.1.34/mysql-connector-java-5.1.34.jar") \  // 可以先不用管
    .getOrCreate()

// 第二步加载数据 ，jdbc 方式 ，具体可以参考api
data = spark.read \
    .jdbc(url=url_local, table="ml_user_credit_summary", predicates=["day = "+ str(day) +" and optType = 1 "],
          properties={"user": user_local, "password": pass_local}).cache()

// 第三步，数据处理
// DataSet api 方式
print(data.select("category").distinct().orderBy("category").collect())

//  sql 方式
data.createOrReplaceTempView("ml_user_credit_summary")
spark.sql("SELECT name, age FROM ml_user_credit_summary WHERE age BETWEEN 13 AND 19").collect()


```


4.2  shell 运行,交互式，调试方便 -- https://spark.apache.org/docs/latest/quick-start.html

.bin/pyspark --master spark://192.168.2.248:40070/ --name test --conf  spark.driver.host=localhost --total-executor-cores 2  --executor-memory 2G  --jars mysql-connector-java-5.1.34.jar

# new
*   下载spark
*   安装python + jupter notebook
*   环境变量:
    *   export PYSPARK_DRIVER_PYTHON=jupyter
    *   export PYSPARK_DRIVER_PYTHON_OPTS=notebook
    *   JAVA_HOME ，HADOOP_HOME ,  CLASSPATH=.%JAVA_HOME%\lib;%JAVA_HOME%\lib\tools.jar
    *   PATH :%JAVA_HOME%\bin及%JAVA_HOME%\jre\bin , %HADOOP_HOME%\bin
    *   SPARK_HOME
    *   PATH :%SPARK_HOME%\bin及%SPARK_HOME%\sbin
    *   其他：export SPARK_CLASSPATH=$ORACLE_HOME/ojdbc8.jar
*   然后直接启动pyspark